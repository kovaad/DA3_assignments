---
title: "DA3_Assignment2_AJK"
author: "Adam Kovacs"
date: "1/28/2022"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE, eval = TRUE)
```

### Introduction

This is a technical report for the project of analyzing data from the [**Inside Airbnb site**](http://insideairbnb.com/get-the-data.html) to help a company operating small and mid-size apartments hosting 2-6 guests price their new real estates with the help of predictive models. The data analyzed consists of estates in Sicily, Italy and was compiled in 28 December, 2021. This document consists of the detailed description of the preprocessing of the data, including all the decisions that were made while crunching the data. 

Afterwards, the modelling part is also provided with technical deetails. A total of 5 models (OLS, LASSO,CART and two random forest) are trained and tested. Finally, they are evaluated and the one that proves to be the best in predicting the price of the outlets is chosen. Finally, the performance of the models are also validated on a holdout sample. 

Note that all codes and data used is available on the [**github repo**](https://github.com/kovaad/DA3_assignments) of my assignments for the Data Analysis 3 course of the MS in Business Analytics at Central European University. 

### Preparatory steps

First we clear the environment, install and load the required packages. Next, we load the raw data. The raw data has 74 columns of 48066 observations. 

```{r import}
#clear memory
rm(list=ls())

#import packages
if (!require(pacman)){
  install.packages("pacman")
}

pacman::p_load(tidyverse, caret, skimr, grid, glmnet, cowplot, modelsummary, fixest, naniar, stringr, kableExtra, xtable)

# import data
data <- read_csv("./data/listings.csv") %>% 
  mutate_if(is.character, factor)

```

### Filtering

After the preparatory steps, it is time to filter the on what we really care about in this project. First, only estates that can accommodate 2 to 6 people are considered. Next, the scope of real estate types to be taken into account for the task needed to be identified. As there was no apartment category, I included all properties that were described either as an entire rental unit, serviced apartment, or home/apartment. Furthermore, I also selected listings that are a (private or shared) room in one of these types that can in themselves accommodate 2 to 6 people. As for some reason some hotel rooms slipped through these filters, they were excluded. 

```{r filtering data}

#filter on 2-6 guests
data <- data %>%
  filter(accommodates %in% seq(2,6,1))

#datasummary(property_type ~ N + Percent(), data = data )
data <- data %>%
  filter(property_type %in% c("Entire rental unit", "Entire serviced apartment",
                              "Entire home/apt", "Private room in rental unit", 
                              "Private room in serviced apartment","Room in rental unit","Room in serviced apartment", "Shared room in rental unit"))

#rename variables, create property type as factor
data <- data %>%
  mutate(
    property_type = ifelse(data$property_type %in% c("Entire rental unit", "Entire serviced apartment", "Entire home/apt"), "Entire apartment", "Room in apartment"),
    f_property_type = factor(property_type))

#datasummary(room_type ~ N + Percent() , data = data )

#there are some hotel rooms - need to be dropped

data <- data %>%
  filter (!room_type %in% c("Hotel room"))

```

Next came the feature engineering part. All variables names are standardized so as to show their type. Only the target is separated, the rest are denoted as starting with f for factors, n for numericals, d for dummies and p for percentages.  

First, the factor variables are transformed this way, such as room type, response type or neighborhood. The bathroom information was only available as text so it had to be separated into a factor on the type (simply bath, private or shared) and a numeric on how many are available. Next, the target and the ratio variables are renamed. This is followed by the handling of the date of posting  the advertisement online and the date of scraping it. These two information are captured as a numeric variable given by simple substraction of the two dates (it gives the days elapsed). Finally, dummy variables needed to be created from the amenities. Since this information was available in a text format, I used grepl to find typical amenities in each listings and gave the value of 1, if there was an each match. Also, some variables available as booleans (e.g. host is superhost, or has profile pic) were also turned into dummy variables. 

Finally, a total of 5 factor, 21 numerical and  48 dummy predictors were created. The dataset was then filtered on these columns, the target and the id. 

```{r feature engineering}

#create room type as factor
data$f_room_type <- factor(ifelse(data$room_type== "Entire home/apt", "Entire/Apt",
                                   ifelse(data$room_type== "Private room", "Private",
                                          ifelse(data$room_type== "Shared room", "Shared", "."))))

#create host response time as categorical - first deal with two ways encoded NA
data <- data %>% naniar::replace_with_na(replace = list(host_response_time = 'N/A'))

data <- data %>%
  mutate(f_host_response_time = factor(host_response_time))

#create neighborhood_cleansed as factor - 367 levels
data <- data %>%
  mutate(f_neighbourhood_cleansed = factor(neighbourhood_cleansed))

#deal with bathrooms text - create numeric and factor variable from it
numextract <- function(string){ 
  str_extract(string, "\\-*\\d+\\.*\\d*")
} 

bathrooms_nr = numextract(data$bathrooms_text)

data <- data %>%
     mutate(bathrooms = as.numeric(numextract(bathrooms_text)), 
            bathroom_type = str_trim(str_split_fixed(data$bathrooms_text, bathrooms_nr, n=2)[,2]))

data <- data %>%
     mutate(bathroom_type = ifelse(data$bathroom_type %in% c("bath", "baths"),"bath", ifelse(data$bathroom_type %in% c("shared bath", "shared baths"), "shared", ifelse(data$bathroom_type  == "private bath", "private", NA))))

data <- data %>%
  mutate(f_bathroom_type = factor(bathroom_type))

#create numeric columns
data <- data %>%
  mutate(
    usd_price_day = as.numeric(price),
    p_host_response_rate = as.numeric(host_response_rate),
    p_host_acceptance_rate = as.numeric(host_acceptance_rate)
  )


numericals <- c("accommodates", "bedrooms", "beds", "minimum_nights", "maximum_nights", "availability_30", "availability_60","availability_90","availability_365", "number_of_reviews","number_of_reviews_ltm", "number_of_reviews_l30d", "review_scores_rating", 
               "host_listings_count", "calculated_host_listings_count", "calculated_host_listings_count_entire_homes", 
                "calculated_host_listings_count_private_rooms", "calculated_host_listings_count_shared_rooms", "reviews_per_month", "bathrooms")


data <- data %>%
  mutate_at(vars(all_of(numericals)), lst("n"=as.numeric))

nnames <- data %>%
  select(ends_with("_n")) %>%
  names()
nnames_i <- match(nnames, colnames(data))
colnames(data)[nnames_i] <- paste0("n_", numericals)


#create days since first review
data <- data %>%
  mutate(
    n_days_since = as.numeric(as.Date(calendar_last_scraped,format="%Y-%m-%d") -
                                as.Date(first_review ,format="%Y-%m-%d")))

#amenities is a list of stuff - use grepl to create dummies

data$essentials <- ifelse(grepl("Essentials",data$amenities),1,0)
data$tv <- ifelse(grepl("TV",data$amenities),1,0)
data$kitchen <- ifelse(grepl("Kitchen",data$amenities),1,0)
data$aircond <- ifelse(grepl("Air conditioning",data$amenities),1,0)
data$hairdry <- ifelse(grepl("Hair dryer",data$amenities),1,0)
data$heating <- ifelse(grepl("Heating",data$amenities),1,0)
data$hotwater <- ifelse(grepl("Hot water",data$amenities),1,0)
data$wifi <- ifelse(grepl("Wifi",data$amenities),1,0)
data$iron <- ifelse(grepl("Iron",data$amenities),1,0)
data$washer <- ifelse(grepl("Washer",data$amenities),1,0)
data$hangers <- ifelse(grepl("Hangers",data$amenities),1,0)
data$longterm <- ifelse(grepl("Long term stays allowed",data$amenities),1,0)
data$balcony <- ifelse(grepl("Patio or balcony",data$amenities),1,0)
data$parking <- ifelse(grepl("Free parking",data$amenities),1,0)
data$parking <- ifelse(grepl("Paid parking",data$amenities),1,0)
data$beachfront <- ifelse(grepl("Beachfront",data$amenities),1,0)
data$freezer <- ifelse(grepl("Freezer|Refrigerator",data$amenities),1,0)
data$kettle <- ifelse(grepl("Hot water kettle",data$amenities),1,0)
data$linens <- ifelse(grepl("Bed linens",data$amenities),1,0)
data$stove <- ifelse(grepl("Stove",data$amenities),1,0)
data$oven <- ifelse(grepl("Oven",data$amenities),1,0)
data$pool <- ifelse(grepl("Pool",data$amenities),1,0)
data$pillows <- ifelse(grepl("Extra pillows and blankets",data$amenities),1,0)
data$fireplace <- ifelse(grepl("Indoor fireplace",data$amenities),1,0)
data$microwave <- ifelse(grepl("Microwave",data$amenities),1,0)
data$workspace <- ifelse(grepl("Dedicated workspace",data$amenities),1,0)
data$cooking <- ifelse(grepl("Cooking basics",data$amenities),1,0)
data$dishes <- ifelse(grepl("Dishes and silverware",data$amenities),1,0)
data$coffee <- ifelse(grepl("Coffee maker",data$amenities),1,0)
data$hostgreets <- ifelse(grepl("Host greets you",data$amenities),1,0)
data$firstaid <- ifelse(grepl("First aid kit",data$amenities),1,0)
data$crib <- ifelse(grepl("Crib",data$amenities),1,0)
data$tub <- ifelse(grepl("Bathtub",data$amenities),1,0)
data$shades <- ifelse(grepl("Room-darkening shades",data$amenities),1,0)
data$backyard <- ifelse(grepl("Backyard",data$amenities),1,0)
data$elevator <- ifelse(grepl("Elevator",data$amenities),1,0)
data$bidet <- ifelse(grepl("Bidet",data$amenities),1,0)
data$wineglass <- ifelse(grepl("Wine glasses",data$amenities),1,0)
data$highchair <- ifelse(grepl("High chair",data$amenities),1,0)
data$bbq <- ifelse(grepl("BBQ grill",data$amenities),1,0)
data$smoke <- ifelse(grepl("Smoke alarm",data$amenities),1,0)
data$luggage <- ifelse(grepl("Luggage dropoff allowed",data$amenities),1,0)
data$toys <- ifelse(grepl("Children\\u2019s books and toys",data$amenities),1,0)

#convert booleans to dummies
data <- data %>% mutate_if(is.logical,as.numeric)

#dummies are the created variables and the booleans that were converted
dummies <- c(c("host_identity_verified", "host_is_superhost", "host_has_profile_pic", "has_availability", "instant_bookable"), names(data)[seq(105,146)])

data <- data %>%
  mutate_at(vars(dummies), funs("d"= (.)))
# rename columns
dnames <- data %>%
  select(ends_with("_d")) %>%
  names()
dnames_i <- match(dnames, colnames(data))
colnames(data)[dnames_i] <- paste0("d_", tolower(gsub("[^[:alnum:]_]", "",dummies)))

# keep columns if contain d_, n_,f_, p_, usd_ and some others
data <- data %>%
  select(matches("^d_.*|^n_.*|^f_.*|^p_.*|^usd_.*"), id)

```


The next part of the feature engineering process was creating functional forms and transform some numeric variables to factors, where it makes more sense that way. For numerical variables such as numer of guests, beds or reviews, squared and logarithmic transforms were created. The number of bathrooms, minimum and maximum nights were turned into factors. 


```{r functional forms}
# Squares and further values to create
data <- data %>%
  mutate(n_accommodates2=n_accommodates^2, 
         ln_accommodates=log(n_accommodates) ,
         ln_accommodates2=log(n_accommodates)^2,
         ln_beds = log(n_beds),
         ln_number_of_reviews = log(n_number_of_reviews+1)
        )

# Pool accomodations with 0,1,2,10 bathrooms
data <- data %>%
  mutate(f_bathroom = cut(n_bathrooms, c(0,1,2,10), labels=c(0,1,2), right = F) )

# Pool num of reviews to 3 categories: none, 1-51 and >51
data <- data %>%
  mutate(f_number_of_reviews = cut(n_number_of_reviews, c(0,1,51,max(data$n_number_of_reviews)), labels=c(0,1,2), right = F))


# Pool and categorize the number of minimum nights: 1,2,3, 3+
data <- data %>%
  mutate(f_minimum_nights= cut(n_minimum_nights, c(1,2,3,max(data$n_minimum_nights)), labels=c(1,2,3), right = F))

#create same for maximum nights, 
data <- data %>%
  mutate(f_maximum_nights= cut(n_maximum_nights, c(0,7,15,31,365, max(data$n_maximum_nights)), labels=c(1,2,3,4,5), right = F))

#check host acceptance rate
#datasummary( p_host_acceptance_rate ~ Mean + Median + Min + Max + P05 + P25 + P75 + P95, data = data )

#replace 101% acceptance rate with 100%
data <- data %>%
  mutate(p_host_acceptance_rate = ifelse(p_host_acceptance_rate == 101, 100, p_host_acceptance_rate))


# Change Infinite values with NaNs
for (j in 1:ncol(data) ) data.table::set(data, which(is.infinite(data[[j]])), j, NA)

```

Now that we have all these variables as possible predictors, it was time to look for and deal with missing data in them. Altogether 19 columns had missing values. First, all observations with missing values in the target variable would have been dropped (there weren't any). For those variables that had a small number of missing values (<20), imputation was used if it could be done in a meaningful manner. For instance,  assuming that where missing, the number of beds is the same as the number of guests or that there is 1 bathroom where it is missing are reasonable imputations. There was one column, capturing the host response time, with more than 7000 missing values that I decided was too much so that it would be redundant to use as predictor, so it was dropped. In three other cases, where there were around 4500 observations missing (from the around 17000), I decided not to drop, but create flag variables, which will be used together with the imputed original column (median values used in place of NAs). These were the number of days elapsed since posting, the review scores rating and the reviews per month. Next, some columns still had 13 missing values that corresponded to the same apartments. Thus, I decided to drop these, as these missing data were systematic, not random. Finally, in case of bath type, those with missing values were categorized into simple "bath". 

In the end of this process, we finally obtained a clean data, with no missing values. This enabled us to create some additional functional forms for numerical variables. This step mostly included the creation of logarithmic forms for the vartiables review scores rating, and days elapsed.  

```{r dealing with NAs}
#check for number of missing values
#to_filter <- sapply(data, function(x) sum(is.na(x)))
#to_filter[to_filter > 0]

#drop if there would be na in target
data <- data %>%
  drop_na(usd_price_day)

#imput when few misssing values
data <- data %>%
  mutate(
    n_bathrooms =  ifelse(is.na(n_bathrooms), median(n_bathrooms, na.rm = T), n_bathrooms), #assume at least 1 bath
    n_beds = ifelse(is.na(n_beds), n_accommodates, n_beds), #assume n_beds=n_accomodates
    f_bathroom=ifelse(is.na(f_bathroom),1, f_bathroom),
    f_minimum_nights=ifelse(is.na(f_minimum_nights),1, f_minimum_nights),
    f_number_of_reviews=ifelse(is.na(f_number_of_reviews),1, f_number_of_reviews),
    ln_beds=ifelse(is.na(ln_beds),0, ln_beds),
    n_bedrooms = ifelse(is.na(n_bedrooms), n_beds, n_bedrooms), #assume n_bedrooms = n_beds
    d_host_has_profile_pic = ifelse(is.na(d_host_has_profile_pic), 0, d_host_has_profile_pic), #if no info on profile pic - good chance not there
    d_host_identity_verified = ifelse(is.na(d_host_identity_verified), 0, d_host_identity_verified),
    d_host_is_superhost = ifelse(is.na(d_host_is_superhost), 0, d_host_is_superhost)
  ) 

# drop the one column that has more than third its values NA - host response time
to_drop <- c("f_host_response_time")
data <- data %>%
  select(-one_of(to_drop))

#to_filter <- sapply(data, function(x) sum(is.na(x)))
#to_filter[to_filter > 0]

# replace missing variables re reviews with zero, when no review + add flags
data <- data %>%
  mutate(
    flag_days_since=ifelse(is.na(n_days_since),1, 0),
    n_days_since =  ifelse(is.na(n_days_since), median(n_days_since, na.rm = T), n_days_since),
    flag_review_scores_rating = ifelse(is.na(n_review_scores_rating),1, 0),
    n_review_scores_rating =  ifelse(is.na(n_review_scores_rating), median(n_review_scores_rating, na.rm = T), n_review_scores_rating),
    flag_reviews_per_month=ifelse(is.na(n_reviews_per_month),1, 0),
    n_reviews_per_month =  ifelse(is.na(n_reviews_per_month), median(n_reviews_per_month, na.rm = T), n_reviews_per_month),
    flag_n_number_of_reviews=ifelse(n_number_of_reviews==0,1, 0)
  )
#table(data$flag_days_since)

#drop if there are only a very little number of values missing (13 from more than 17000)
data <- data %>%
  drop_na(p_host_response_rate)

#correct missing categorical variables in f_maximum_nights
#View(data[is.na(data$f_maximum_nights),c("n_maximum_nights", "f_maximum_nights")])
data <- data %>%
  mutate(
    f_maximum_nights = ifelse(is.na(f_maximum_nights) & (n_maximum_nights < 7),1, ifelse(is.na(f_maximum_nights) & (n_maximum_nights == 222020), 4, f_maximum_nights)))


#if bathroom type na, put in simple category of bath
data$f_bathroom_type[is.na(data$f_bathroom_type)] <- "bath"

# Create variables, measuring the time since: squared, cubic, logs
data <- data %>%
  mutate(
    ln_days_since = log(n_days_since+1),
    ln_days_since2 = log(n_days_since+1)^2,
    ln_days_since3 = log(n_days_since+1)^3 ,
    n_days_since2=n_days_since^2,
    n_days_since3=n_days_since^3,
    ln_review_scores_rating = log(n_review_scores_rating),
    ln_days_since=ifelse(is.na(ln_days_since),0, ln_days_since),
    ln_days_since2=ifelse(is.na(ln_days_since2),0, ln_days_since2),
    ln_days_since3=ifelse(is.na(ln_days_since3),0, ln_days_since3),
  )

```


After the completion of the feature engineering the label, that is the price of the apartments was also looked at in detail. Based on the descriptive statistics, there are some apartments where staying a night costs unreasonably little money. Since there are only a handful of such observations (.. to be precise), we decided to exclude them from the analysis for two main reasons. First, they seem to be errors, it seems to be unimaginanble to spend a night in Sicily for 10 USD, it is far too little money (a lunch costs more). Second, they would make the prediction interval wider, and since in the live data we do not expect such low prices, we do not need to account for such variation.  

```{r label engineering}

P95 <- function(x){quantile(x,0.95,na.rm=T)}
P05 <- function(x){quantile(x,0.05,na.rm=T)}

#datasummary( usd_price_day ~ Mean + Median + Min + Max + P05 + P25 + P75 + P95, data = data )

#maybe take log
data <- data %>%
  mutate(ln_usd_price_day = log(usd_price_day))

#filter on at least 98 USD, even for a room below that is unreasonably low
data <- data %>%
  filter(usd_price_day <1000 & usd_price_day >50)

```

The distribution of the target variable is displayed on Figure 1. Interestingly, there are a lot of apartments charging a daily fee of somewhat above 100 USD, which seems to form a standard for the cheap apartments. Above this category, there are somewhat less apartments up until around 350 - 500 USD, where spikes indicate the prevelance of substantial share of apartments being offered. Finally, the third category where a larger share of the apartments are offered falls within the 650-800 USD price range. 

The possibility of a log transformation was considered, but it did not make the distribution of the outcome variable better, and we are interested in predicting the price, so it would have only introduced more complication and negligible (if any) gain. 


```{r distribution of target, fig.height = 2, fig.width = 4, fig.align='center', fig.cap="Distribution of target variable", fig.pos='htb!'}

ggplot(data=data, aes(x=usd_price_day)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 30, boundary=0,
                  fill = 'navyblue', color = 'white', size = 0.25, alpha = 0.8,  show.legend=F,  na.rm=TRUE) +
  coord_cartesian(xlim = c(50, 900)) +
  labs(x = "Price (US dollars)",y = "Percent")+
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.09), breaks = seq(0, 0.09, by = 0.03), labels = scales::percent_format(1)) +
    scale_x_continuous(expand = c(0.05,0.00),limits=c(100,900), breaks = seq(100,900, 100)) +
  theme_bw() 
```

Finally, we arrive at the end of the data preprocessing, cleaning and filtering phase. Let us look at some descriptives in the form of summary tables. As we can see, the final number of observations remained rather high at 17350. This should be enough to carry out even more sophisticated machine learning algorithms such as the random forest. 


```{r final check and save clean data}
# Look at data
datasummary( id ~ N , data = data )
datasummary_skim( data , 'categorical' )


# where do we have missing variables now?
#to_filter <- sapply(data, function(x) sum(is.na(x)))
#to_filter[to_filter > 0]

# N=17350
#write_csv(data, "./data/airbnb_sicily_clean.csv")

```




```{r read in and descriptives}

data <- read_csv("./data/airbnb_sicily_clean.csv")

# Descriptive statistics
datasummary( f_room_type + f_property_type + f_bathroom_type ~ N + Percent() , data = data )

```

```{r histogram}

ggplot(data=data, aes(x=usd_price_day)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 30, boundary=0,
                  fill = 'navyblue', color = 'white', size = 0.25, alpha = 0.8,  show.legend=F,  na.rm=TRUE) +
  coord_cartesian(xlim = c(50, 900)) +
  labs(x = "Price (US dollars)",y = "Percent")+
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.09), breaks = seq(0, 0.09, by = 0.03), labels = scales::percent_format(1)) +
    scale_x_continuous(expand = c(0.05,0.00),limits=c(100,900), breaks = seq(100,900, 100)) +
  theme_bw() 
```

```{r boxplots}
ggplot(data = data, aes(x = f_room_type, y = usd_price_day)) +
  stat_boxplot(aes(group = f_room_type), geom = "errorbar", width = 0.3,
               color = c('red','blue', 'black'), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_room_type),
               color = c('red','blue', 'black'), fill = c('red','blue', 'black'),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(x = "Room type",y = "Price (US dollars)")+
  theme_bw()
```





```{r}

ggplot(data=data, aes(x=ln_usd_price_day)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.18,
                 color = 'white', fill = 'navyblue', size = 0.25, alpha = 0.8,  show.legend=F,  na.rm=TRUE) +
  coord_cartesian(xlim = c(4.2, 6.5)) +
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.15, by = 0.05), labels = scales::percent_format(5L)) +
  scale_x_continuous(expand = c(0.00,0.01),breaks = seq(4,7, 0.5)) +
  labs(x = "ln(price, US dollars)",y = "Percent")+
  theme_bw() 

```

```{r start modelling}

# create train and holdout samples

set.seed(2801)
train_indices <- as.integer(createDataPartition(data$usd_price_day, p = 0.7, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]

# Check the number of observations
dim(data_train)
dim(data_holdout)

# Define models: simpler -> extended

# Basic Variables inc neighnourhood
basic_vars <- c(
  "n_accommodates", "n_beds", "n_days_since",
  "f_property_type","f_room_type", "n_bathrooms", "f_bathroom_type",
  "f_neighbourhood_cleansed")

less_basic_vars <- c("f_minimum_nights", "f_maximum_nights", 
  "n_availability_30", "n_availability_60","n_availability_90", "n_availability_365")

# reviews
reviews <- c("n_number_of_reviews", "flag_n_number_of_reviews" ,
             "n_review_scores_rating", "flag_review_scores_rating",
             "n_reviews_per_month", "flag_reviews_per_month", 
             "n_number_of_reviews_ltm","n_number_of_reviews_l30d")
# host infos
host <- c("p_host_response_rate","p_host_acceptance_rate", 
          "n_calculated_host_listings_count", "n_calculated_host_listings_count_entire_homes",
          "n_calculated_host_listings_count_private_rooms", "n_calculated_host_listings_count_shared_rooms")

# Dummy variables
amenities <-  grep("^d_.*", names(data), value = TRUE)

#interactions for the LASSO
X1  <- c("n_accommodates*f_property_type",  "f_room_type*f_property_type",
         "d_aircond*f_property_type", "d_tv*f_property_type", "d_longterm*f_property_type")
# with boroughs
X2  <- c("f_property_type*f_neighbourhood_cleansed", "f_room_type*f_neighbourhood_cleansed",
         "n_accommodates*f_neighbourhood_cleansed" )

predictors_1 <- c(basic_vars)
predictors_2 <- c(basic_vars, less_basic_vars, reviews, host, amenities)
predictors_E <- c(basic_vars, less_basic_vars, reviews, host, amenities, X1,X2)

```


```{r random forest}

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# set tuning
tune_grid <- expand.grid(
  .mtry = c(8),
  .splitrule = "variance",
  .min.node.size = c(50)
)


# simpler model for model - using random forest
set.seed(1234)
system.time({
rf_model_1 <- train(
  formula(paste0("usd_price_day ~", paste0(predictors_1, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})


# more complicated model - using random forest
set.seed(1234)
system.time({
rf_model_2 <- train(
  formula(paste0("usd_price_day ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})
rf_model_2

#evaluating rf models

results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2
  )
)
summary(results)

```


```{r model diagnostics}


rf_model_2_var_imp <- ranger::importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_2_var_imp_df

ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```




```{r horserace: compare with other models }

# OLS with dummies for area
# using model B

set.seed(1234)
system.time({
ols_model <- train(
  formula(paste0("usd_price_day ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "lm",
  trControl = train_control
)
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

# * LASSO
# using extended model w interactions

set.seed(1234)
system.time({
lasso_model <- train(
  formula(paste0("usd_price_day ~", paste0(predictors_E, collapse = " + "))),
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
  trControl = train_control
)
})

lasso_coeffs <- coef(
    lasso_model$finalModel,
    lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `s1`)  # the column has a name "1", to be renamed

lasso_coeffs_non_null <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]

regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null, by = "variable", all=TRUE)

# CART with built-in pruning
set.seed(1234)
system.time({
cart_model <- train(
  formula(paste0("usd_price_day ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "rpart",
  tuneLength = 10,
  trControl = train_control
)
})
cart_model

#GBM

gbm_grid <-  expand.grid(interaction.depth = 5, # complexity of the tree
                         n.trees = 250, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)


set.seed(1234)
system.time({
  gbm_model <- train(formula(paste0("usd_price_day ~", paste0(predictors_2, collapse = " + "))),
                     data = data_train,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model
gbm_model$finalModel

```

```{r Model selection}
# get prediction rmse and add to next summary table
# ---- compare these models

final_models <-
  list("OLS" = ols_model,
  "LASSO (model w/ interactions)" = lasso_model,
  "CART" = cart_model,
  "Random forest 1: smaller model" = rf_model_1,
  "Random forest 2: extended model" = rf_model_2,
  "GBM"  = gbm_model)

results <- resamples(final_models) %>% summary()
results

# Model selection is carried out on this CV RMSE
result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

result_4
```

```{r evaluate on holdout}

mod2$xlevels[["y"]] <- union(mod2$xlevels[["y"]], levels(data_holdout$y))

result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout), data_holdout[["usd_price_day"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

result_5

```




