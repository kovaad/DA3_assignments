---
title: "DA3 - Assignment 1"
author: "Adam Kovacs"
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.5cm
fontsize: 8pt
output: 
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
header-includes: |
  \usepackage{titling}
  \setlength{\droptitle}{-8em}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE)
```


### Introduction

In this project, I analyze the [**cps-earnings dataset**](https://osf.io/g8p9j/) to build predictive models targeting the earnings per hour of advertising, promotions, marketing and sales managers. Altogether 4 models are built, all OLS, but increasing in complexity. The performance of these models are evaluated through RMSE and BIC in the full sample and through cross-validated RMSE. Finally, the relationship between model complexity and performance is illustrated with the help of a visual aid. 

```{r import}
#clear memory
rm(list=ls())

#import packages
if (!require(pacman)){
  install.packages("pacman")
}

pacman::p_load(tidyverse, modelsummary, kableExtra, fixest, estimatr, caret, stargazer, lmtest, sandwich)

# import data
df_orig <- read_csv( 'https://osf.io/4ay9x/download' )

```


```{r filter}

# keep only two occupation types: Advertising and promotions managers and 
# Marketing and sales managers
df_orig <- df_orig %>% mutate(sample=ifelse(occ2012==0040,1,
                                              ifelse(occ2012 == 0050,2,0)))

df <- df_orig %>%
  filter(sample==1 | sample==2) 
```

### Preparatory steps

Having filtered the dataset to the chosen occupations, the first task was label engineering. The required target variable is earnings per hour, but we have data on weekly earnings and usual weekly working hours. Thus, I calculated the target variable from these two through a simple division. Next, I looked at the descriptive statistics of earnings per hour to look for extreme values. I detected an extremely low minimum value and since earning such a low wage is not even allowed by law, I decided to filter on at least 1 USD hourly wage (losing one observation). See the descriptive table after the drop in Table 1 of the Appendix. 

```{r label engineering}

#create target variable
df <- df %>% mutate(w = earnwke/uhours)
# filter out extreme values -- hourly wage should be at least 1 USD -- lower does not make sense (1 observation lost)

df <- df %>% filter(w >= 1)

```

Next, we move on to feature engineering. From the categorical variable on highest education, we create 6 dummies, namely for those that did not finish high school, finished high school, have an associate degree, have a bachelors degree, have a masters degree and have a professional degree. Dummies are created also for gender (female as 1), for union membership, whether the person is native, whether the workplace is in the private sector and the employment status. 
As for functional forms, an important numerical variable where this is important is age. Between age and earnings, both earlier literature and the data suggests a non-linear, quadratic relationship, so we create that feature as well. 

```{r feature engineering}

#Data refactoring

#create 6 dummies for highest education
df <- df %>%
  mutate(no_high_school = ifelse(grade92 %in% seq(31,38,1), 1, 0),
         high_schol = ifelse(grade92 == 39 | grade92 == 40, 1,0),
         associate_degree = ifelse(grade92 == 41 | grade92 == 42, 1,0),
         bachelors = ifelse(grade92 == 43,1,0),
         masters = ifelse(grade92 == 44, 1, 0),
         professional = ifelse(grade92 == 45, 1, 0))

#create female dummy
df <- df %>%
  mutate(female = as.numeric(sex == 2))

#create union member dummy
df <- df %>%
  mutate(union = ifelse(unionmme == "No",0,1))

#create native dummy
df <- df %>%
  mutate(native = ifelse(prcitshp == "Native, Born In US" | prcitshp == "Native, Born Abroad Of US Parent(s)",1,0))

#create private sector dummy
df <- df %>%
  mutate(private = ifelse(class == "Private, For Profit" | prcitshp == "Private, Nonprofit",1,0))

#create employed_at_work dummy
df <- df %>%
  mutate(employed_at_work = ifelse(lfsr94 == "Employed-At Work",1,0))

# age: quadratic
df <- df %>%
  mutate(agesq = age^2)

```

### Modelling and evaluation

Finally we arrive at modelling. In the first, simplest model, only one variable age (and its square) is used as predictors, which are good proxies for experience. In the second model, we include the traditional university degrees (bachelors and masters) that are also highly valued by firms when deciding on wages. In the third model, besides adding professional degree as well, important characteristics of people (gender, race, union membership) and firms are also added (whether it is in private sector). Finally, in the most complex model, all highest education dummies and further personal traits are included (marital status, number of children, whether they are native).

```{r models}
# create linear regression models with increasing complexity

model1 <- as.formula(w ~ age + agesq)
model2 <- as.formula(w ~ age + agesq + bachelors + masters)
model3 <- as.formula(w ~ age + agesq + bachelors + masters + professional + female + race + union + private)
model4 <- as.formula(w ~ age + agesq + bachelors + masters + female + race + union + marital + ownchild + native + private + employed_at_work + no_high_school + high_schol + associate_degree + professional)

```

First, we evaluate the models using the full sample. Based on both the Akaike and Bayesian information criterion, the best model is the third one, which has the lowest number. Looking at the RMSE, however, it has its minimum at the fourth, most complex model across the four models estimated.

We also consider a more robust evaluation method through a 4-fold cross-validation. Based on the average RMSE of the four folds, it leans towards the suggestion of the information criterions: the third model has the lowest value, beating the fourth one by quite some margin. 

The relationship between model complexity and the performance of the models is also illustrated through a visual aid in the Appendix. We can see that going from the first to the second model improved the predictive power substantially, university degree seems to be indeed very important. Adding the further features describing the characteristics of the person and the firm improved the RMSE further in the third model. However, adding all remaining predictors resulted in a worse performance after all, most likely due to some bad, unnecessary predictors.

### Conclusion

To conclude, in this exercise we used the [**cps-earnings dataset**](https://osf.io/g8p9j/) to predict the hourly wage of two types of workers: advertising & promotions managers and marketing & sales managers. Four models were built with increasing complexity containing 2, 4, 8 and 16 predictors. Based on the full-sample RMSE, the most complex one performed the best. However, based on the BIC and AIC as well as the average RMSE after a 4-fold cross-validation, the less complex third model turned out to have the best predictive power.  

\pagebreak

### Appendix

```{r descriptive table target}
#create 5th and 95th percentiles

P95 <- function(x){quantile(x,0.95,na.rm=T)}
P05 <- function(x){quantile(x,0.05,na.rm=T)}

#create descriptive table
datasummary( (`Hourly wage` = w ) ~
               Mean + Median + SD + Min + Max + P05 + P95, 
             data = df,
             title = 'Hourly wage of Advertising and Sales Managers' ) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

```


```{r regression, results = "asis"}

# EXTRA: for writing out with stargazer: use lm instead
reg1 <- lm(model1, data=df)
reg2 <- lm(model2, data=df)
reg3 <- lm(model3, data=df)
reg4 <- lm(model4, data=df)
# evaluation of the models
models <- c("reg1", "reg2","reg3", "reg4")
AIC <- c()
BIC <- c()
RMSE <- c()
RSquared <- c()
regr <- c()
k <- c()
# Get for all models
for ( i in 1:length(models)){
  AIC[i] <- AIC(get(models[i]))
  BIC[i] <- BIC(get(models[i]))
  RMSE[i] <- RMSE(predict(get(models[i])), get(models[i])$model$w)
  RSquared[i] <-summary(get(models[i]))$r.squared
  regr[[i]] <- coeftest(get(models[i]), vcov = sandwich)
  k[i] <- get(models[i])$rank -1
}

```


```{r eval, results = "asis"}
eval <- data.frame(models, k, RSquared, RMSE, BIC)
eval <- eval %>%
  mutate(models = paste0("(",gsub("reg","",models),")")) %>%
  rename(Model = models, "R-squared" = RSquared, "Training RMSE" = RMSE, "N predictors" = k)
stargazer(eval, summary = F, digits=2, float = T, no.space = T, type = "latex", header=FALSE, title = 'Summary of evaluation on full sample')

#stargazer(reg1, reg2, reg3, reg4 , align = T,   digits=2, dep.var.caption = "Dep. var: w", keep.stat = c("rsq","n"), title = "Wage - regression", no.space = T, type = "latex", header=FALSE)
```

```{r evaluation cv, results = "asis"}

# set number of folds to use
k <- 4

# create four folds
set.seed(13505)
cv1 <- train(model1, df, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(13505)
cv2 <- train(model2, df, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(13505)
cv3 <- train(model3, df, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")
set.seed(13505)
cv4 <- train(model4, df, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

# Calculate RMSE for each fold and average RMSE as well
cv <- c("cv1", "cv2", "cv3", "cv4")
rmse_cv <- c()

for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                       get(cv[i])$resample[[1]][2]^2 +
                       get(cv[i])$resample[[1]][3]^2 +
                       get(cv[i])$resample[[1]][4]^2)/4)
}


# summarize results
cv_mat <- data.frame(rbind(cv1$resample[4], "Average"),
           rbind(cv1$resample[1], rmse_cv[1]),
           rbind(cv2$resample[1], rmse_cv[2]),
           rbind(cv3$resample[1], rmse_cv[3]),
           rbind(cv4$resample[1], rmse_cv[4])
           )

colnames(cv_mat)<-c("Resample","Model1", "Model2", "Model3", "Model4")
stargazer(cv_mat, summary = F, digits=2, float=T, type = "latex", header=FALSE, title = 'Summary of cross-validation obtained RMSEs')

```


```{r model complexity, fig.height = 3, fig.width = 4, fig.align='center', fig.cap="Prediction performance and model compexity", fig.pos='htb!'}

# Show model complexity and out-of-sample RMSE performance

m_comp <- c()
models <- c("reg1", "reg2", "reg3", "reg4")
for( i in 1 : length(cv) ){
  m_comp[ i ] <- length( get( models[i] )$coefficient  - 1 ) 
}

m_comp <- tibble( model = models , 
                  complexity = m_comp,
                  RMSE = rmse_cv )

ggplot( m_comp , aes( x = complexity , y = RMSE ) ) +
  geom_point(color='red',size=2) +
  geom_line(color='blue',size=0.5)+
  labs(x='Number of explanatory variables',y='Averaged RMSE on test samples') +
  theme_bw() 
```



