---
title: "DA3 - Assignment 1"
author: "Adam Kovacs"
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.5cm
fontsize: 8pt
output: 
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
header-includes: |
  \usepackage{titling}
  \setlength{\droptitle}{-8em}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE)
```


### Introduction

In this project, I analyze the [**cps-earnings dataset**](https://osf.io/g8p9j/) to build predictive models targeting the earnings per hour of advertising, promotions, marketing and sales managers. Altogether 4 models are built, all OLS, but increasing in complexity. The performance of these models are evaluated through RMSE and BIC in the full sample and through cross-validated RMSE. Finally, the relationship between model complexity and performance is illustrated with the help of a visual aid. 

```{r import}
#clear memory
rm(list=ls())

#import packages
if (!require(pacman)){
  install.packages("pacman")
}

pacman::p_load(tidyverse, modelsummary, kableExtra, fixest, estimatr, caret, stargazer, lmtest, sandwich)

# import data
df_orig <- read_csv( 'https://osf.io/4ay9x/download' )

```


```{r filter}

# keep only two occupation types: Advertising and promotions managers and 
# Marketing and sales managers
df_orig <- df_orig %>% mutate(sample=ifelse(occ2012==0040,1,
                                              ifelse(occ2012 == 0050,2,0)))

df <- df_orig %>%
  filter(sample==1 | sample==2) 
```

### Preparatory steps

Having filtered the dataset to the chosen occupations, label engineering was needed. The required target variable is earnings per hour, but we have data on weekly earnings and usual weekly working hours. Thus, I calculated the target variable from these two through a simple division. Next, descriptive statistics of earnings per hour were looked at to identify potential extreme values. I detected an extremely low value and since earning such a low wage is not even allowed by law, a filter of at least 1 USD hourly wage was applied (losing one observation). Descriptive statistics after filtering is available in Table 1 of the Appendix. 

```{r label engineering}

#create target variable
df <- df %>% mutate(w = earnwke/uhours)
# filter out extreme values -- hourly wage should be at least 1 USD -- lower does not make sense (1 observation lost)

df <- df %>% filter(w >= 1)

```

Next came feature engineering: From the categorical variable on highest education,  6 dummies were created, namely did not finish high school, finished high school, have an associate degree, a bachelors degree, a masters degree or professional degree. Dummies are created also for gender (female as 1), for union membership, whether the person is native, whether the workplace is in the private sector and the employment status. 
As for functional forms, an important numerical variable where this is important is age. Between age and earnings, the lowess on Table 2 of the Appendix suggests a non-linear, quadratic relationship, so we create age squared as a feature as well. 

```{r feature engineering}

#Data refactoring

#create 6 dummies for highest education
df <- df %>%
  mutate(no_high_school = ifelse(grade92 %in% seq(31,38,1), 1, 0),
         high_schol = ifelse(grade92 == 39 | grade92 == 40, 1,0),
         associate_degree = ifelse(grade92 == 41 | grade92 == 42, 1,0),
         bachelors = ifelse(grade92 == 43,1,0),
         masters = ifelse(grade92 == 44, 1, 0),
         professional = ifelse(grade92 == 45, 1, 0))

#create female dummy
df <- df %>%
  mutate(female = as.numeric(sex == 2))

#create union member dummy
df <- df %>%
  mutate(union = ifelse(unionmme == "No",0,1))

#create native dummy
df <- df %>%
  mutate(native = ifelse(prcitshp == "Native, Born In US" | prcitshp == "Native, Born Abroad Of US Parent(s)",1,0))

#create private sector dummy
df <- df %>%
  mutate(private = ifelse(class == "Private, For Profit" | prcitshp == "Private, Nonprofit",1,0))

#create employed_at_work dummy
df <- df %>%
  mutate(employed_at_work = ifelse(lfsr94 == "Employed-At Work",1,0))

# age: quadratic
df <- df %>%
  mutate(agesq = age^2)

```

### Modelling and evaluation

As noted above, four models are built. In the first, simplest model, only one variable, age (and its square) are used as predictors, which are good proxies for experience. In the second model, we include the traditional university degrees (bachelors and masters) that are also highly valued by firms when deciding on wages. In the third model, besides adding professional degree as well, important characteristics of people (gender, race, union membership) and firms are also added (whether it is in private sector). Finally, in the most complex model, all (seemingly less valuable) highest education dummies and further personal traits are also included (marital status, number of children, whether they are native).

```{r models}
# create linear regression models with increasing complexity

model1 <- as.formula(w ~ age + agesq)
model2 <- as.formula(w ~ age + agesq + bachelors + masters)
model3 <- as.formula(w ~ age + agesq + bachelors + masters + professional + female + race + union + private)
model4 <- as.formula(w ~ age + agesq + bachelors + masters + female + race + union + marital + ownchild + native + private + employed_at_work + no_high_school + high_schol + associate_degree + professional)

```

First, we evaluate the models using the full sample. The exact values are displayed on Table 2 of the Appendix. Based on the Bayesian information criterion, the best model is the third one, which has the lowest number. Looking at the RMSE, however, it has its minimum at the fourth, most complex model.

A more robust evaluation method is also looked at, using 4-fold cross-validation. Based on the average RMSE of the four folds, it leans towards the suggestion of the RMSE in the full sample, not the BIC. The fourth model has the lowest value, beating the third one by quite some margin (14.87 compared to 14.95). 

The relationship between model complexity and the performance of the models is also illustrated through a visual aid on Figure 2 of the Appendix. Going from the first to the second model improved the predictive power substantially: traditional university degree seems to be indeed very important. Next, adding further features describing the characteristics of people and the firm improved the RMSE further, but to a lesser extent. Finally, adding all remaining predictors resulted in a slightly even better performance after all, but the tendency of smaller improvement with higher complexity is evident. Adding even more functional forms/interaction terms could easily even worsen out-of-sample performance. 

### Conclusion

To conclude, the task was predicting the hourly wage of advertising & promotions managers and marketing & sales managers. Four models were built with increasing complexity containing 2, 4, 9 and 16 predictors respectively. Based on the full-sample and average cross-validated RMSE, the most complex one performed the best. However, based on the BIC, the less complex third model turned out to have the best predictive power.

\pagebreak

### Appendix

```{r descriptive table target}
#create 5th and 95th percentiles

P95 <- function(x){quantile(x,0.95,na.rm=T)}
P05 <- function(x){quantile(x,0.05,na.rm=T)}

#create descriptive table
datasummary( (`Hourly wage` = w ) ~
               Mean + Median + SD + Min + Max + P05 + P95, 
             data = df,
             title = 'Hourly wage of Advertising and Sales Managers' ) %>% 
  kableExtra::kable_styling(latex_options = "hold_position")

```

```{r lowess for age, fig.height = 3, fig.width = 4, fig.align='center', fig.cap="Relationship between age and hourly wage", fig.pos='htb!'}
# lowess for age and hourly wage
ggplot(data = df, aes(x=age, y=w)) +
  geom_point( color = "red", size = 1) + 
  geom_smooth(method="loess", se=F, colour="blue", size=1, span=0.9) +
  labs(x = "Age (years)",y = "Hourly wage (US dollars)") +
  theme_bw() 

```


```{r regression, results = "asis"}

# EXTRA: for writing out with stargazer: use lm instead
reg1 <- lm(model1, data=df)
reg2 <- lm(model2, data=df)
reg3 <- lm(model3, data=df)
reg4 <- lm(model4, data=df)
# evaluation of the models
models <- c("reg1", "reg2","reg3", "reg4")
AIC <- c()
BIC <- c()
RMSE <- c()
RSquared <- c()
regr <- c()
k <- c()
# Get for all models
for ( i in 1:length(models)){
  AIC[i] <- AIC(get(models[i]))
  BIC[i] <- BIC(get(models[i]))
  RMSE[i] <- RMSE(predict(get(models[i])), get(models[i])$model$w)
  RSquared[i] <-summary(get(models[i]))$r.squared
  regr[[i]] <- coeftest(get(models[i]), vcov = sandwich)
  k[i] <- get(models[i])$rank -1
}

```


```{r eval, results = "asis"}
eval <- data.frame(models, k, RSquared, RMSE, BIC)
eval <- eval %>%
  mutate(models = paste0("(",gsub("reg","",models),")")) %>%
  rename(Model = models, "R-squared" = RSquared, "Training RMSE" = RMSE, "N predictors" = k)

stargazer(eval, summary = F, digits=2, float = T, no.space = T, type = "latex", header=FALSE, title = 'Summary of evaluation on full sample')

```

```{r evaluation cv, results = "asis"}

# set number of folds to use
k <- 4

# create four folds
set.seed(13505)
cv1 <- train(model1, df, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(13505)
cv2 <- train(model2, df, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(13505)
cv3 <- train(model3, df, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")
set.seed(13505)
cv4 <- train(model4, df, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

# Calculate RMSE for each fold and average RMSE as well
cv <- c("cv1", "cv2", "cv3", "cv4")
rmse_cv <- c()

for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                       get(cv[i])$resample[[1]][2]^2 +
                       get(cv[i])$resample[[1]][3]^2 +
                       get(cv[i])$resample[[1]][4]^2)/4)
}


# summarize results
cv_mat <- data.frame(rbind(cv1$resample[4], "Average"),
           rbind(cv1$resample[1], rmse_cv[1]),
           rbind(cv2$resample[1], rmse_cv[2]),
           rbind(cv3$resample[1], rmse_cv[3]),
           rbind(cv4$resample[1], rmse_cv[4])
           )

colnames(cv_mat)<-c("Resample","Model1", "Model2", "Model3", "Model4")

stargazer(cv_mat, summary = F, digits=2, float=T, type = "latex", header=FALSE, title = 'Summary of cross-validation obtained RMSEs')

```


```{r model complexity, fig.height = 3, fig.width = 4, fig.align='center', fig.cap="Prediction performance and model compexity", fig.pos='htb!'}

# Show model complexity and out-of-sample RMSE performance

m_comp <- c()
models <- c("reg1", "reg2", "reg3", "reg4")
for( i in 1 : length(cv) ){
  m_comp[ i ] <- length( get( models[i] )$coefficient  - 1 ) 
}

m_comp <- tibble( model = models , 
                  complexity = m_comp,
                  RMSE = rmse_cv )

ggplot( m_comp , aes( x = complexity , y = RMSE ) ) +
  geom_point(color='red',size=2) +
  geom_line(color='blue',size=0.5)+
  labs(x='Number of explanatory variables',y='Averaged RMSE on test samples') +
  theme_bw() 
```




